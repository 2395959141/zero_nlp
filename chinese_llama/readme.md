# 介绍
基于`llama`模型框架从零训练一个中文大模型

## 背景
1. `facebook`泄漏出来的`llama`模型，不能商用。而且也没中文token，虽然很多国内大牛已经对其进行扩充了。
2. `chatglm-6b`模型权重也不能商用，但是对中文的支持能力，太顶了！
3. 于是想想，能不能把`chatglm-6b`的`tokenizer`直接拿过来，嫁接到任意一个开源的大模型框架上，并基于这个大模型框架，从零训练一个中文模型。
4. 同时为了解决数据不知道怎么处理的问题，我这边也基于`belle`给到的数据，整理了一份`491万`条的数据文件。


