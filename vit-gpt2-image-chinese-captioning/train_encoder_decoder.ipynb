{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/yuanzhoulvpi/gpt2_chinese\n",
    "# https://huggingface.co/google/vit-base-patch16-224\n",
    "# https://huggingface.co/nlpconnect/vit-gpt2-image-captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (VisionEncoderDecoderModel, \n",
    "                          BertConfig, ViTConfig,\n",
    "                          ViTModel, GPT2Config, GPT2Model,GPT2LMHeadModel,\n",
    "                          AutoTokenizer,ViTImageProcessor,\n",
    "                          Trainer,TrainingArguments)\n",
    "from typing import List, Any \n",
    "import torch\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from datasets import load_dataset,Dataset\n",
    "\n",
    "from tqdm import tqdm \n",
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIT_MODEL_NAME_OR_PATH = \"google/vit-base-patch16-224\"\n",
    "GPT_MODEL_NAME_OR_PATH = \"yuanzhoulvpi/gpt2_chinese\"\n",
    "\n",
    "\n",
    "VIT_model = ViTModel.from_pretrained(VIT_MODEL_NAME_OR_PATH)\n",
    "GPT_model = GPT2LMHeadModel.from_pretrained(GPT_MODEL_NAME_OR_PATH, add_cross_attention=True)\n",
    "\n",
    "GPT_model.config.add_cross_attention# = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ViTImageProcessor.from_pretrained(VIT_MODEL_NAME_OR_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(GPT_MODEL_NAME_OR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_2_pixel_value(x:str) -> Tensor:\n",
    "    image = Image.open(x)\n",
    "    res = processor(images=image, return_tensors='pt')['pixel_values'].squeeze(0)\n",
    "    return res \n",
    "\n",
    "\n",
    "process_image_2_pixel_value(x = \"bigdata/image_data/test-9282.jpg\").shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_2_input_id(x:str) :\n",
    "    res = tokenizer(text=x,max_length=100, truncation=True,padding=\"max_length\")['input_ids']\n",
    "    return res \n",
    "\n",
    "len(process_text_2_input_id(x='hhh'))\n",
    "\n",
    "# len(process_text_2_input_id(x=\"你好啊，csdhhchsh谁cdshhchshcsdhhhhhhhh\")['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT_model.config.add_cross_attention = True\n",
    "# # GPT_model.crossattention = False\n",
    "# GPT_model.config.add_cross_attention\n",
    "\n",
    "# # config.add_cross_attention=True\n",
    "# # hasattr(GPT_model, \"crossattention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_encoder_decoder_model = VisionEncoderDecoderModel(\n",
    "    encoder=VIT_model,\n",
    "    decoder=GPT_model,\n",
    "    \n",
    ")\n",
    "# new_encoder_decoder_model.config.use_return_dict = False\n",
    "new_encoder_decoder_model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "new_encoder_decoder_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# new_encoder_decoder_model.decoder.config.add_cross_attention=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(process_text_2_input_id(x='hhh'), dtype=torch.long).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_encoder_decoder_model.config.add_cross_attention = True\n",
    "new_encoder_decoder_model.config.add_cross_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = new_encoder_decoder_model(pixel_values=torch.randn(1, 3,224,224), \n",
    "                                    labels=torch.tensor(process_text_2_input_id(x='hhh'), dtype=torch.long).unsqueeze(0),\n",
    "                                    # return_dict=False\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_encoder_decoder_model.config.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.add_cross_attention\n",
    "# new_encoder_decoder_model.decoder.config.add_cross_attention = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_encoder_decoder_model.config.decoder_start_token_id is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Tensor([1,2,3]).to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df=pd.read_csv(\"bigdata/clean_train_test/test.csv\"))\n",
    "dataset = dataset.train_test_split(test_size=0.02)\n",
    "\n",
    "\n",
    "def tokenizer_text(examples) :\n",
    "    examples['labels'] = [process_text_2_input_id(i) for i in examples['text']]\n",
    "    # res = [process_text_2_input_id(i) for i in examples['text']]\n",
    "    # examples['labels'] = [i['input_ids'] for i in res]\n",
    "    return examples\n",
    "\n",
    "def transform_images(examples):\n",
    "    images = [process_image_2_pixel_value(i) for i in examples['image_path']]\n",
    "    # images = [torch.Tensor(i) for i in images]\n",
    "    examples['pixel_values'] = images\n",
    "    return examples\n",
    "\n",
    "dataset = dataset.map(\n",
    "    function=tokenizer_text,\n",
    "    batched=True\n",
    ")\n",
    "# dataset = dataset.map(\n",
    "#     function=transform_images,\n",
    "#     batched=True\n",
    "# )\n",
    "\n",
    "dataset.set_transform(transform=transform_images)\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([i['pixel_values'] for i in examples])\n",
    "    labels = torch.tensor([example[\"labels\"] for example in examples], dtype=torch.long)\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "\n",
    "train_argument = TrainingArguments(\n",
    "    output_dir=\"vit-gpt2-image-chinese-captioning\",\n",
    "    per_device_train_batch_size=28,\n",
    "    per_device_eval_batch_size=28,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=30,\n",
    "    logging_steps=30,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=30,\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False,\n",
    "    save_total_limit=4\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=new_encoder_decoder_model,\n",
    "    args=train_argument,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]['pixel_values'].numpy().ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]['pixel_values'].squeeze(0).numpy().ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mynet2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "110bc624a448454d574a0cd6cc76359fd86f75739e493913b3d71c2e04f2ffdb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
